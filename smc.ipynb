{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adaa5c0",
   "metadata": {},
   "source": [
    "# `S`um of `M`inimums of `C`onvex \n",
    "\n",
    "<b>author</b>: @guiguiom \n",
    "\n",
    "<b>package name</b>: SMC\n",
    "\n",
    "<b>package type</b>: [CVXPY](https://www.cvxpy.org) extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3500d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic imports \n",
    "import numbers\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from cvxpy.expressions.expression import Expression\n",
    "from cvxpy.expressions.constants import Constant,Parameter\n",
    "from scipy.special import softmax\n",
    "import itertools\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6303a2",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027ab4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Mathieu Blondel\n",
    "# License: BSD 3 clause\n",
    "\n",
    "\n",
    "def proj_simplex_vec(V, z=1):\n",
    "    n_features = V.shape[1]\n",
    "    U = np.sort(V, axis=1)[:, ::-1]\n",
    "    z = np.ones(len(V)) * z\n",
    "    cssv = np.cumsum(U, axis=1) - z[:, np.newaxis]\n",
    "    ind = np.arange(n_features) + 1\n",
    "    cond = U - cssv / ind > 0\n",
    "    rho = np.count_nonzero(cond, axis=1)\n",
    "    theta = cssv[np.arange(len(V)), rho - 1] / rho\n",
    "    return np.maximum(V - theta[:, np.newaxis], 0)\n",
    "\n",
    "def proj_simplex(v, z=1):\n",
    "    n_features = v.shape[0]\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u) - z\n",
    "    ind = np.arange(n_features) + 1\n",
    "    cond = u - cssv / ind > 0\n",
    "    rho = ind[cond][-1]\n",
    "    theta = cssv[cond][-1] / float(rho)\n",
    "    w = np.maximum(v - theta, 0)\n",
    "    return w\n",
    "\n",
    "# from https://github.com/cvxgrp/dccp/blob/master/dccp/linearize.py\n",
    "\n",
    "def linearize(expr):\n",
    "    \"\"\"Returns the tangent approximation to the expression.\n",
    "\n",
    "    Gives an elementwise lower (upper) bound for convex (concave)\n",
    "    expressions. No guarantees for non-DCP expressions.\n",
    "\n",
    "    Args:\n",
    "        expr: An expression.\n",
    "\n",
    "    Returns:\n",
    "        An affine expression.\n",
    "    \"\"\"\n",
    "    if expr.is_affine():\n",
    "        return expr\n",
    "    else:\n",
    "        if expr.value is None:\n",
    "            raise ValueError(\n",
    "                \"Cannot linearize non-affine expression with missing variable values.\"\n",
    "            )\n",
    "        tangent = np.real(expr.value) #+ np.imag(expr.value)\n",
    "        grad_map = expr.grad\n",
    "        for var in expr.variables():\n",
    "            if grad_map[var] is None:\n",
    "                return None\n",
    "            complex_flag = False\n",
    "            if var.is_complex() or np.any(np.iscomplex(grad_map[var])):\n",
    "                complex_flag = True\n",
    "            if var.ndim > 1:\n",
    "                temp = cp.reshape(\n",
    "                    cp.vec(var - var.value), (var.shape[0] * var.shape[1], 1)\n",
    "                )\n",
    "                if complex_flag:\n",
    "                    flattened = np.transpose(np.real(grad_map[var])) @ cp.real(temp) + \\\n",
    "                    np.transpose(np.imag(grad_map[var])) @ cp.imag(temp)\n",
    "                else:\n",
    "                    flattened = np.transpose(np.real(grad_map[var])) @ temp\n",
    "                tangent = tangent + cp.reshape(flattened, expr.shape)\n",
    "            elif var.size > 1:\n",
    "                if complex_flag:\n",
    "                    tangent = tangent + np.transpose(np.real(grad_map[var])) @ (cp.real(var) - np.real(var.value)) \\\n",
    "                    + np.transpose(np.imag(grad_map[var])) @ (cp.imag(var) - np.imag(var.value))\n",
    "                else:\n",
    "                    tangent = tangent + np.transpose(np.real(grad_map[var])) @ (var - var.value)\n",
    "            else:\n",
    "                if complex_flag:\n",
    "                    tangent = tangent + np.real(grad_map[var]) * (cp.real(var) - np.real(var.value)) \\\n",
    "                    + np.imag(grad_map[var]) * (cp.imag(var) - np.imag(var.value))\n",
    "                else:\n",
    "                    tangent = tangent + np.real(grad_map[var]) * (var - var.value) \n",
    "        return tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ee7e1",
   "metadata": {},
   "source": [
    "### objective creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07c9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "### \n",
    "### \n",
    "# new class\n",
    "### \n",
    "### \n",
    "### \n",
    "\n",
    "class MinExpr:\n",
    "    \"\"\"\n",
    "    class used to represent the minimum of cvxpy expressions \n",
    "    --------------------------------------------------------\n",
    "    \n",
    "    Attributes: \n",
    "    \n",
    "    expr_array : list or 2d-array of CONVEX cvxpy Expression \n",
    "        LIST\n",
    "        | every Expression must exhibit same dimension \n",
    "        | within each Expression, a single dtype is allowed\n",
    "    dim : int\n",
    "        | represents the \"depth\"/\"dimension\" of Expression objects stored in expr_array\n",
    "    NOTES:\n",
    "        \n",
    "        ARRAY entry at (pos1,pos2) <=> LIST element at (pos2) regarding dimension pos1\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def __init__(self, expr_array):\n",
    "        self.expressions = None\n",
    "        if isinstance(expr_array,Expression): # array case\n",
    "            self.expressions = expr_array.copy()\n",
    "        elif isinstance(expr_array,list): # list case \n",
    "            try:\n",
    "                self.expressions = cp.vstack(expr_array.copy()).T\n",
    "                assert isinstance(self.expressions,Expression),'one argument passed does not match cvxpy Expression format'\n",
    "            except:\n",
    "                print(\"error while creating MinExpr object:: check dimensions of list content arguments\")\n",
    "        else:\n",
    "            print(\"expr_array should be either a cvxpy Expression array or a list of Expression of equivalent sizes\")\n",
    "\n",
    "        if len(self.expressions.shape)<2:\n",
    "            self.dim = 1\n",
    "        elif len(self.expressions.shape)==2:\n",
    "            self.dim = self.expressions.shape[0]\n",
    "            if self.dim==1: # flatten() equivalent\n",
    "                self.expressions = self.expressions[0]\n",
    "        else: \n",
    "            print(\"expr_array must be a 1 or 2 dimensional\")\n",
    "       \n",
    "        assert self.expressions.is_convex(), \"expr must be convex\"\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"evaluates min_l=1...n of Expr_l for each pos1\"\"\"\n",
    "        if self.dim==1:\n",
    "            return np.min(self.expressions.value)\n",
    "        else: \n",
    "            return np.min(self.expressions.value,1)\n",
    "        \n",
    "    @property\n",
    "    def extended_values(self):\n",
    "        \"\"\"evaluates every component\"\"\"\n",
    "        return self.expressions.value\n",
    "    \n",
    "    @property \n",
    "    def ns(self):\n",
    "        if self.dim==1:\n",
    "            return [int(np.prod(self.expressions.shape))]\n",
    "        return self.dim*[int(self.expressions.shape[1])]\n",
    "    \n",
    "    def __add__(self, e):\n",
    "        if isinstance(e, MinExpr):\n",
    "            return SumMinExpr(list_min_exprs=[self,e])\n",
    "        elif isinstance(e, SumMinExpr):\n",
    "            return e+self\n",
    "        elif isinstance(e, numbers.Number):\n",
    "            self.expressions += Constant(e)\n",
    "            return self\n",
    "        elif isinstance(e, Expression):\n",
    "            assert e.is_convex(),'e should be a cvxpy CONVEX Expression'\n",
    "            try:\n",
    "                self.expressions += e\n",
    "            except:\n",
    "                print('dimensions mismatch')\n",
    "            return self\n",
    "        else:\n",
    "            raise ValueError(\"type %s not supported in __add__\" % type(e))\n",
    "            \n",
    "    __radd__ = __add__\n",
    "    \n",
    "    def __mul__(self, e):\n",
    "        \"\"\"multiplies the MinExpr to another object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        e : numbers.Number or np.ndarray (POSITIVE) or CONSTANT/PARAMETER\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if e is not a supported type.\n",
    "        Returns\n",
    "        -------\n",
    "        SumMinExpr\n",
    "        \"\"\"\n",
    "        assert isinstance(e, numbers.Number) or isinstance(e,np.ndarray) or isinstance(e,Constant) or isinstance(e,Parameter),'e should be a scalar or an np.ndarray'\n",
    "        if ((isinstance(e,Constant) or isinstance(e,Parameter)) and e.is_nonneg()) or (isinstance(e, numbers.Number) and e>=0):\n",
    "            try:\n",
    "                self.expressions = cp.multiply(e,self.expressions)\n",
    "            except:\n",
    "                raise ValueError('dimension not matching between e and self.expressions')\n",
    "            return self\n",
    "        elif np.min(e)>=0:\n",
    "            try:\n",
    "                self.expressions = cp.multiply(Constant(e),self.expressions)\n",
    "            except:\n",
    "                raise ValueError('dimension not matching between e and self.expressions')\n",
    "            return self\n",
    "        else:\n",
    "            raise ValueError(\"type %s not supported in __add__\" % type(e))\n",
    "            \n",
    "    __rmul__ = __mul__ \n",
    "    \n",
    "    def clip(self,lamb):\n",
    "        \"\"\"takes the minimum between a MinExpr and a scalar lamb\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lamb : numbers.Number \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        MinExpr\n",
    "        \"\"\"\n",
    "        assert isinstance(lamb,numbers.Number),'lamb should be a number'\n",
    "        if self.dim>1:\n",
    "            self.expressions = cp.hstack((self.expressions,lamb*np.ones((self.dim,1))))\n",
    "        else:\n",
    "            self.expressions = cp.hstack((self.expressions,lamb))\n",
    "            \n",
    "    def compress(self,weights=None):\n",
    "        \"\"\"produces the cvxpy CONVEX Expression sum_l=1...ns weight_l*Expr_l\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : None (default) or np.ndarray\n",
    "            | weights being positive and of length ns\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Expression \n",
    "        \"\"\"\n",
    "        loc_ns = self.ns[0]\n",
    "        if weights is None:\n",
    "            weights = np.ones(loc_ns)/loc_ns\n",
    "        if self.dim==1:\n",
    "            assert len(weights)==loc_ns and (np.array(weights)>=0).all(),'weights should be a positive vector of length ns'\n",
    "            sw_ = sum(weights)\n",
    "            return np.array(weights)/sw_@self.expressions\n",
    "        else:\n",
    "            assert weights.shape == (self.dim,loc_ns) and (np.array(weights)>=0).all(),'weights should be a positive matrix of size (self.dim,ns)'\n",
    "            sw_ = np.sum(weights,1)\n",
    "            return cp.sum(cp.multiply(np.array(weights)/np.outer(np.ones(self.dim),sw_),self.expressions))\n",
    "        \n",
    "        \n",
    "    def param_expand(self):\n",
    "        \"\"\"produces the parametric cvxpy CONVEX Expression sum_l=1...ns weight_l*Expr_l\n",
    "           with newly instanciated cvxpy POSITIVE parameters weight_l for l=1...ns\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Expression (parametric)\n",
    "        \"\"\"\n",
    "        new_param = cp.Parameter(self.expressions.shape,nonneg=True)\n",
    "        return cp.sum(cp.multiply(new_param,self.expressions)),[new_param]\n",
    "    \n",
    "### \n",
    "### \n",
    "### \n",
    "# new class\n",
    "### \n",
    "### \n",
    "### \n",
    "\n",
    "    \n",
    "class SumMinExpr:\n",
    "    \"\"\"\n",
    "    class used to represent a sum of MinExpr\n",
    "    ----------------------------------------\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "    main_expr: Expression\n",
    "        | CONVEX cvxpy Expression that stands outside Min operators (see doc.)\n",
    "        \n",
    "    Methods:\n",
    "\n",
    "    __add__(e)\n",
    "        -> adds the object to an Expression, MinExpr, SumOfMinExpr, or numbers.Number\n",
    "    \"\"\"\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def __init__(self, list_min_exprs,main_fun=None):\n",
    "        if main_fun is not None:\n",
    "            assert isinstance(main_fun,Expression),'main (common) expression should be a cvxpy Expression'\n",
    "            assert main_fun.is_convex(),'main (common) expression should be convex'\n",
    "            self.main_expr = main_fun.copy()\n",
    "        else:\n",
    "            self.main_expr = Constant(0.0)\n",
    "        self.min_exprs = list_min_exprs\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def num_exprs(self):\n",
    "        'returns N in our formulation of SMC'\n",
    "        return sum([me.dim for me in self.min_exprs])\n",
    "    \n",
    "    @property\n",
    "    def ns_list(self):\n",
    "        buf = []\n",
    "        for me in self.min_exprs:\n",
    "            buf.append(me.ns)\n",
    "        return buf\n",
    "\n",
    "    def __add__(self, e):\n",
    "        \"\"\"adds the SumMinExpr to another object\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        e : Expression, MinExpr, SumMinExpr, or numbers.Number\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            if e is not a supported type.\n",
    "        Returns\n",
    "        -------\n",
    "        SumMinExpr\n",
    "        \"\"\"\n",
    "        if isinstance(e, MinExpr):\n",
    "            self.min_exprs += [e]\n",
    "            return self\n",
    "        elif isinstance(e, SumMinExpr):\n",
    "            self.min_exprs += e.min_exprs\n",
    "            self.main_expr += e.main_expr\n",
    "            return self\n",
    "        elif isinstance(e, numbers.Number):\n",
    "            self.main_expr += Constant(e)\n",
    "            return self\n",
    "        elif isinstance(e, Expression):\n",
    "            assert e.is_convex(),'e should be a cvxpy CONVEX Expression'\n",
    "            self.main_expr += e\n",
    "            return self\n",
    "        else:\n",
    "            raise ValueError(\"type %s not supported in __add__\" % type(e))\n",
    "            \n",
    "    __radd__ = __add__\n",
    "    \n",
    "    \n",
    "    def __mul__(self, e):\n",
    "        try:\n",
    "            self.main_expr *= Constant(e)\n",
    "            for me in self.min_exprs:\n",
    "                me *= e\n",
    "        except:\n",
    "            raise ValueError('dimension not matching between e and either self.main_expr or one of the expr')\n",
    "        return self\n",
    "            \n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    \n",
    "    def clip(self, lamb):\n",
    "        \"\"\"clips every term of a SumMinExpr to scalar value lamb\n",
    "        \"\"\"\n",
    "        assert isinstance(lamb,numbers.Number),'lamb should be a number'\n",
    "        for me in self.min_exprs:\n",
    "            me.clip(lamb)\n",
    "    \n",
    "    @property \n",
    "    def value(self):\n",
    "        \"\"\"overall value\"\"\"\n",
    "        buf = self.main_expr.value\n",
    "        for me in self.min_exprs:\n",
    "            buf += np.sum(me.value)\n",
    "        return buf\n",
    "    \n",
    "    @property\n",
    "    def extended_values(self):\n",
    "        return [me.extended_values for me in self.min_exprs]\n",
    "    \n",
    "    def active(self,relax=1e-5):\n",
    "        assert relax>=0 and relax<=1,'relax must be in [0,1]'\n",
    "        ext_val = self.extended_values\n",
    "        ext_indices = []\n",
    "        if relax==0:\n",
    "            for vals in ext_val:\n",
    "                indices = []\n",
    "                for row in vals:\n",
    "                    indices.append(np.argmin(row))\n",
    "                ext_indices.append(indices)\n",
    "        else:\n",
    "            for vals in ext_val:\n",
    "                indices = []\n",
    "                minvals = np.min(vals,1)\n",
    "                maxvals = np.max(vals,1)\n",
    "                shifted_vals = (np.outer(maxvals,np.ones(vals.shape[1]))-np.array(vals))/(np.outer(maxvals,np.ones(vals.shape[1]))-np.outer(minvals,np.ones(vals.shape[1])))\n",
    "                for row in shifted_vals:\n",
    "                    indices.append(list(np.where(row>=1-relax)[0]))\n",
    "                ext_indices.append(indices)\n",
    "        return ext_indices\n",
    "                                                                                    \n",
    "    def compress(self,weights_list):\n",
    "        comp = self.main_expr\n",
    "        assert len(weights_list)==self.num_exprs,'there should be as many weighting factors as MinExpr stored in SumMinExpr object'\n",
    "        for me,weights in zip(self.min_exprs,weights_list):\n",
    "            comp += me.compress(weights)\n",
    "        return comp\n",
    "    \n",
    "    def param_expand(self):\n",
    "        comp = self.main_expr\n",
    "        new_param_list = []\n",
    "        for me in self.min_exprs:\n",
    "            new_term,new_param = me.param_expand()\n",
    "            comp += new_term\n",
    "            new_param_list += new_param\n",
    "        return comp,new_param_list\n",
    "\n",
    "    \n",
    "    # TO DO .visualize(variable_values)\n",
    "    \n",
    "### \n",
    "### \n",
    "### \n",
    "# static method @todo add Expressions handle with lamb\n",
    "### \n",
    "### \n",
    "### \n",
    "def minimum(e,lamb):\n",
    "    assert isinstance(lamb,numbers.Number) or isinstance(lamb,Parameter) or isinstance(lamb,Constant),'lamb should be a number or cp.Parameter/Constant'\n",
    "    assert isinstance(e, Expression),'e should be a cvxpy Expression'\n",
    "    if isinstance(lamb,numbers.Number):\n",
    "        if len(e.shape)<2:\n",
    "            return SumMinExpr([MinExpr(cp.hstack((cp.reshape(e,(e.shape[0],1)),lamb*np.ones((e.shape[0],1)))))])\n",
    "        else:\n",
    "            return SumMinExpr([MinExpr(cp.hstack((e,lamb*np.ones((e.shape[0],1)))))])\n",
    "    else:\n",
    "        assert len(lamb.shape)==1 and lamb.shape[0]==1,'lamb should be 1d'\n",
    "        if len(e.shape)<2:\n",
    "            return SumMinExpr([MinExpr(cp.hstack((cp.reshape(e,(e.shape[0],1)),lamb*np.ones((e.shape[0],1)))))])\n",
    "        else:\n",
    "            return SumMinExpr([MinExpr(cp.hstack((e,lamb*np.ones((e.shape[0],1)))))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99465320",
   "metadata": {},
   "source": [
    "### Problem Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7342aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    \"\"\"\n",
    "    minimizing a SumMinExpr\n",
    "    --------------------------------------------\n",
    "    \n",
    "    Attributes:\n",
    "\n",
    "    objective : SumMinExpr\n",
    "    constraints : list\n",
    "        | list of cvxpy constraints\n",
    "    vars_ : list\n",
    "        | cvxpy Variables (pointers) involved in the problem\n",
    "    custom_param_expand : \n",
    "        | @comment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, objective, constraints=[],custom_param_expand=None,custom_dc_expand=None):\n",
    "        if isinstance(objective, SumMinExpr):\n",
    "            self.objective = objective\n",
    "        elif isinstance(objective,cp.Minimize):\n",
    "            assert objective.is_dcp(),'objective should be DCP'\n",
    "            self.objective = SumMinExpr(list_min_exprs=[],main_fun=objective.expr)\n",
    "        elif isinstance(objective,cp.Maximize):\n",
    "            assert objective.is_dcp(),'objective should be DCP'\n",
    "            self.objective = SumMinExpr(list_min_exprs=[],main_fun=-objective.expr)\n",
    "        else:\n",
    "            raise ValueError('objective should either be a valid cvxpy DCP Objective or SumMinExpr')\n",
    "        self.constraints = constraints\n",
    "        for cstr in self.constraints:\n",
    "            assert cstr.is_dcp(),'constraints must be CONVEX'\n",
    "        self.vars_ = []\n",
    "        for min_expr in self.objective.min_exprs:\n",
    "            self.vars_ += min_expr.expressions.variables()\n",
    "        self.vars_ += self.objective.main_expr.variables()\n",
    "        for constr in self.constraints:\n",
    "            self.vars_ += constr.variables()\n",
    "        self.vars_ = list(set(self.vars_))\n",
    "        if custom_dc_expand is None:\n",
    "            self.custom_dc_decomp = False\n",
    "        else:\n",
    "            self.custom_dc_decomp = True\n",
    "            self.dc_obj_fun_generator = custom_dc_expand\n",
    "        if custom_param_expand is None:\n",
    "            self.custom_set = False\n",
    "            self.param_obj_fun,self.param_pointers_list = self.objective.param_expand()\n",
    "        else:\n",
    "            self.custom_set = True\n",
    "            self.param_obj_fun,self.param_pointers_list,self.w2p = custom_param_expand[0],custom_param_expand[1],custom_param_expand[2]\n",
    "\n",
    "\n",
    "    def solve(self, method=\"max-min\", *args, **kwargs):\n",
    "        \"\"\"approximately solve the problem \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str\n",
    "            | see paper\n",
    "        args, kwargs\n",
    "        \"\"\"\n",
    "        if method in ['max-min','am','boyd','softmin']:\n",
    "            return self._solve_RAM(mode=method,*args, **kwargs)\n",
    "        elif method=='RInDCAe':\n",
    "            return self._solve_RInDCAe(*args, **kwargs)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def weights_setup(self,mode='equiv'):\n",
    "        weights = []\n",
    "        if mode=='equiv':\n",
    "            for num_s in self.objective.ns_list:\n",
    "                if len(num_s)==1:\n",
    "                    weights.append(np.ones(num_s[0])/num_s[0])\n",
    "                else:\n",
    "                    weights.append(np.outer(np.ones(len(num_s)),np.ones(num_s[0])/num_s[0]))\n",
    "        elif mode=='random':\n",
    "            for num_s in self.objective.ns_list:\n",
    "                if len(num_s)==1:\n",
    "                    base_weight = np.random.uniform(0,1,num_s[0])\n",
    "                    weights.append(base_weight/sum(base_weight))\n",
    "                else:\n",
    "                    base_weight = np.random.uniform(0,1,(len(num_s),num_s[0]))\n",
    "                    weights.append(base_weight/np.outer(np.sum(base_weight,1),np.ones(num_s[0]))) \n",
    "        elif mode=='random_int':\n",
    "            for num_s in self.objective.ns_list:\n",
    "                if len(num_s)==1:\n",
    "                    base_weight = np.zeros(num_s[0])\n",
    "                    base_weight[np.random.choice(nums_s[0])] += 1\n",
    "                    weights.append(base_weight)\n",
    "                else:\n",
    "                    base_weight = np.zeros((len(num_s),num_s[0]))\n",
    "                    for _ in range(len(num_s)):\n",
    "                        base_weight[_,np.random.choice(num_s[0])]+=1\n",
    "                    weights.append(base_weight) \n",
    "        return weights\n",
    "    \n",
    "    def weights_update(self,mode,weights,coefficient,param=1.0):\n",
    "        vals_list = self.objective.extended_values\n",
    "        new_weights = []\n",
    "        for weight,vals in zip(weights,vals_list):\n",
    "            if len(vals.shape)<2:\n",
    "                wcand = np.zeros(vals.shape)\n",
    "                minid = np.argmin(vals)\n",
    "                minval = vals[minid]\n",
    "                w_star = np.zeros(vals.shape)\n",
    "                w_star[minid] = minval\n",
    "                ####### CANDIDATE ELECTION #######\n",
    "                if mode=='am' or coefficient<1e-3:\n",
    "                    w_hat = w_star\n",
    "                elif mode=='max-min':\n",
    "                    maxval = np.max(vals)\n",
    "                    w_hat = proj_simplex(param*(maxval-vals)/(maxval-minval))\n",
    "                elif mode=='boyd':\n",
    "                    gbar = np.array(vals)-minval\n",
    "                    sgbar = np.sign(gbar)\n",
    "                    w_hat = weight-param*sgbar\n",
    "                    w_hat[minid] += param*(1+sgbar[minid])\n",
    "                    w_hat = proj_simplex(w_hat)\n",
    "                else: \n",
    "                    w_hat = softmax(-param*vals/max(1e-4,np.mean(vals))) # vandessel\n",
    "                ####### epsilon - SG #######\n",
    "                num = np.sum((weight-w_star)*vals)\n",
    "                denum = np.sum((w_hat-w_star)*vals)\n",
    "                if coefficient*num>=denum:\n",
    "                    combination = 1\n",
    "                else:\n",
    "                    combination = min(1,max(0,coefficient*(num/max(1e-9,denum))))\n",
    "                wcand = combination*w_hat+(1-combination)*w_star\n",
    "                combs = [combination]\n",
    "            elif len(vals.shape)==2:\n",
    "                wcand = np.zeros(vals.shape)\n",
    "                minids = np.argmin(vals,1)\n",
    "                minvals = np.array([vals[_,mid] for _,mid in enumerate(minids)])  \n",
    "                w_star = np.zeros(vals.shape)\n",
    "                for _,mid in enumerate(minids):\n",
    "                    w_star[_,mid] += 1\n",
    "                ####### CANDIDATE ELECTION #######\n",
    "                if mode=='am' or coefficient<1e-3:\n",
    "                    w_hat = w_star\n",
    "                elif mode=='max-min':\n",
    "                    maxvals = np.max(vals,1)\n",
    "                    w_hat = proj_simplex_vec(param*(np.outer(maxvals,np.ones(vals.shape[1]))-np.array(vals))/(np.outer(maxvals,np.ones(vals.shape[1]))-np.outer(minvals,np.ones(vals.shape[1]))))\n",
    "                elif mode=='boyd':\n",
    "                    gbar = np.array(vals)-np.outer(minvals,np.ones(vals.shape[1]))\n",
    "                    sgbar = np.sign(gbar)\n",
    "                    w_hat = weight-param*np.sign(gbar)\n",
    "                    for _,mid in enumerate(minids):\n",
    "                        w_hat[_,mid] += param*(1+sgbar[_,mid])\n",
    "                    w_hat = proj_simplex_vec(w_hat)\n",
    "                else: \n",
    "                    w_hat = softmax(-param*vals/np.maximum(1e-4,np.abs(np.outer(np.mean(vals,1),np.ones(vals.shape[1])))),axis=1)\n",
    "                ####### epsilon - SG #######\n",
    "                combs = []\n",
    "                for _,w_hat_elem in enumerate(w_hat):\n",
    "                    num = np.sum((weight[_]-w_star[_])*vals[_])\n",
    "                    denum = np.sum((w_hat_elem-w_star[_])*vals[_])\n",
    "                    if coefficient*num>=denum:\n",
    "                        combination = 1\n",
    "                    else:\n",
    "                        combination = min(1,max(0,coefficient*(num/max(1e-9,denum))))\n",
    "                    wcand[_] = combination*w_hat_elem+(1-combination)*w_star[_]\n",
    "                    combs.append(combination)\n",
    "            else:\n",
    "                print('ValueError: wrong evaluation of component functions... please check dimensions')\n",
    "            new_weights.append(wcand)\n",
    "        return new_weights\n",
    "    \n",
    "    def _init_variables(self,extra_verb_=False,warm_start=False, warm_start_weights=None,init_weights='equiv', **kwargs):\n",
    "        #### goal -> create init. x just as it would appear for RAM methods <- ####\n",
    "        if warm_start_weights is not None:\n",
    "            weights = warm_start_weights.copy()\n",
    "        else:\n",
    "            if isinstance(init_weights,str):\n",
    "                weights = self.weights_setup(mode=init_weights)\n",
    "            else:\n",
    "                try:\n",
    "                    weights = init_weights(self)\n",
    "                    # TO DO full assertive statements \n",
    "                    assert sum([len(weight) for weight in weights])==self.objective.num_exprs,'user prescribed weights should match objective SumMinExpr.num_exprs'\n",
    "                    for elem in weights:\n",
    "                        if len(elem.shape)==1:\n",
    "                            assert np.min(elem)>=-1e-8 and abs(1-np.sum(elem))<=1e-8,'every weight vector should be POSITIVE and sum up to 1'\n",
    "                        elif len(elem.shape)==2:\n",
    "                            assert np.min(elem)>=-1e-8 and np.max(np.abs(1-np.sum(elem,1)))<=1e-8,'every weight vector should be POSITIVE and sum up to 1'\n",
    "                        else:\n",
    "                            raise ValueError('tensor like parameters not accepted yet...')\n",
    "                except:\n",
    "                    print('init_weights as a callable should take a smc.Problem argument | equiv. weights loaded...')\n",
    "                    weights = self.weights_setup()\n",
    "        \n",
    "        cvx_param_obj_prob = cp.Problem(cp.Minimize(self.param_obj_fun), self.constraints)\n",
    "        \n",
    "        if not warm_start:\n",
    "            try:\n",
    "                ## param affectation + solve\n",
    "                if self.custom_set:\n",
    "                    params = self.w2p(weights)\n",
    "                    for param,param_pointer in zip(params,self.param_pointers_list):\n",
    "                        param_pointer.value = param\n",
    "                else:\n",
    "                    for weight,param_pointer in zip(weights,self.param_pointers_list):\n",
    "                        param_pointer.value = np.maximum(0,weight)\n",
    "            except:\n",
    "                print('numerical instability for weights/params setting...')\n",
    "            try:\n",
    "                cvx_param_obj_prob.solve(**kwargs)\n",
    "                if extra_verb_:\n",
    "                    print('-> init ok.')\n",
    "                return cvx_param_obj_prob\n",
    "            except:\n",
    "                print('solver issues... or manual STOP')\n",
    "                print(cvx_param_obj_prob.value)\n",
    "                return None\n",
    "\n",
    "            if cvx_param_obj_prob.status in ['unbounded','infeasible']:\n",
    "                raise ValueError(\"weights-fixed problem is %s.\" % cvx_param_obj_prob.status)\n",
    "        else:\n",
    "            print('problem already warm-started...')\n",
    "            return None\n",
    "    \n",
    "    def _solve_RInDCAe(self,maxIters=50,verb_=False,extra_verb_=False,tol=1e-9,total_strong_convex_param=0,\n",
    "                warm_start=False, warm_start_weights=None,init_weights='equiv',kappa=1.0,TOL_has_moved=1e-3, **kwargs):\n",
    "        \n",
    "        \"\"\" (Refined Inertial DC Algorithm with extrapolation)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        @COMPLETE\n",
    "        **kwargs\n",
    "            | keyword arguments to be sent to cvxpy solve() function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        info : dict\n",
    "            | dictionary of solver information\n",
    "        \"\"\"\n",
    "        \n",
    "        assert kappa>=0,'positive kappa required'\n",
    "        \n",
    "        _ = self._init_variables(extra_verb_,warm_start,warm_start_weights,init_weights, **kwargs)\n",
    "        \n",
    "        var_previous = [var.value for var in self.vars_]\n",
    "        var_actual = [var.value for var in self.vars_]\n",
    "        DIVERGENCE_new = 0.0\n",
    "        \n",
    "        out_val = []\n",
    "        \n",
    "        true_val = np.inf\n",
    "        \n",
    "        pre_mu_add = max(0,kappa-total_strong_convex_param/2)\n",
    "        mu_add = 2*pre_mu_add \n",
    "        div_coef = mu_add+total_strong_convex_param-kappa\n",
    "        \n",
    "        print('kappa = '+str(kappa))\n",
    "        print('added mu = '+str(mu_add))\n",
    "           \n",
    "        if self.custom_dc_decomp==False:\n",
    "            f1_obj = self.objective.main_expr\n",
    "            for min_expr in self.objective.min_exprs:\n",
    "                f1_obj += cp.sum(min_expr.expressions)\n",
    "            \n",
    "        for k in range(maxIters):\n",
    "            \n",
    "            DIVERGENCE_previous = DIVERGENCE_new\n",
    "            \n",
    "            active_list = self.objective.active(0)\n",
    "            \n",
    "            if k>0:\n",
    "                var_previous = var_actual.copy()\n",
    "                var_actual = [var.value for var in self.vars_]\n",
    "            \n",
    "            if self.custom_dc_decomp==False:\n",
    "\n",
    "                f2_lin_obj = 0.0\n",
    "\n",
    "                for active_indices,min_expr in zip(active_list,self.objective.min_exprs):\n",
    "                    lin_models = linearize(min_expr.expressions)\n",
    "                    f2_lin_obj = cp.sum(lin_models)\n",
    "                    for idsel,selected in enumerate(active_indices):\n",
    "                        f2_lin_obj -= lin_models[idsel,selected]\n",
    "\n",
    "                fobj = f1_obj-f2_lin_obj\n",
    "                \n",
    "            else:\n",
    "                fobj = self.dc_obj_fun_generator(active_list)\n",
    "                \n",
    "            if kappa>0 and k>0: # extrapolation mechanism\n",
    "                for varid,var in enumerate(self.vars_):\n",
    "                    fobj -= kappa*cp.sum(cp.multiply(var_actual[varid]-var_previous[varid],var))\n",
    "                \n",
    "            if mu_add: # correction for theory safeguard\n",
    "                for var in self.vars_:\n",
    "                    fobj += mu_add/2*cp.sum_squares(var)-cp.sum(cp.multiply(mu_add*var.value,var-var.value))\n",
    "                    \n",
    "            prob = cp.Problem(cp.Minimize(fobj), self.constraints)\n",
    "            if extra_verb_:\n",
    "                print('convexified problem created')\n",
    "            try:\n",
    "                prob.solve(**kwargs)\n",
    "                if extra_verb_:\n",
    "                    print('convexified problem solved')\n",
    "            except:\n",
    "                print('solver issues... or manual STOP')\n",
    "                    \n",
    "            DIVERGENCE_new = 0.0\n",
    "            if div_coef>0:\n",
    "                for va,vp in zip(var_actual,var_previous):\n",
    "                    if isinstance(va,numbers.Number):\n",
    "                        DIVERGENCE_new += div_coef/2*(va-vp)**2\n",
    "                    else:\n",
    "                        DIVERGENCE_new += div_coef/2*np.sum((va-vp)**2)\n",
    "                    \n",
    "            last_val = true_val\n",
    "            true_val = self.objective.value\n",
    "            out_val.append(true_val)\n",
    "\n",
    "            if verb_:\n",
    "                print(\"iter. %04d | Fval. %4.4e \" % (k + 1, true_val))\n",
    "                if extra_verb_ and (k>0 or not warm_start):\n",
    "                    print('variables: '+str([var.value for var in self.vars_]))\n",
    "                    \n",
    "            decr = (last_val+DIVERGENCE_previous)-(true_val+DIVERGENCE_new)\n",
    "                \n",
    "            if decr < tol and true_val<out_val[0]-TOL_has_moved:\n",
    "                if verb_:\n",
    "                    print (\"-> terminated (stopping condition satisfied)\")\n",
    "                break\n",
    "            else:\n",
    "                TOL_has_moved = max(-1e-9,TOL_has_moved*2/3-1e-9)\n",
    "                \n",
    "        if verb_ and k == maxIters-1:\n",
    "            print (\"-> terminated (maximum number of iterations reached)\")\n",
    "            \n",
    "        return {'iters':k+1,'stopping_condition':decr,'objective_values':out_val}   \n",
    "    \n",
    "    \n",
    "####\n",
    "#### (w*a+w_star*(1-a))^T h <= w_star^T h + Ck* (w_last-w_star)^T h\n",
    "####  a <=  Ck*((w_last-w_star)^T h)/((w-w_star)^T h) \n",
    "####\n",
    "    \n",
    "    def _solve_RAM(self,mode='am',maxIters=50,verb_=False,extra_verb_=False,tol=1e-9,\n",
    "                warm_start=False, warm_start_weights=None,init_weights='equiv',kappa=1.0,C_sched=None,param_sched=None,TOL_has_moved=1e-3, **kwargs):\n",
    "        \n",
    "        \"\"\" (Relaxed Alternating Minimization)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        maxIters : int\n",
    "            | maximum number of iterations (default = 50)\n",
    "        tol : double\n",
    "            | numerical tolerance for stopping condition (default = 1e-9)\n",
    "        verb_ : bool\n",
    "            | whether or not to print information (default = False)\n",
    "        warm_start : bool\n",
    "            | whether or not some value affectation has already been conducted on Problem's variables vars_\n",
    "        warm_start_weights : np.ndarray\n",
    "            | choice bias; warm start value for a priori weights (default = None)\n",
    "        init_weights : str or callable\n",
    "            | 'random','equiv' or homemade init technique\n",
    "        **kwargs\n",
    "            | keyword arguments to be sent to cvxpy solve() function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        info : dict\n",
    "            | dictionary of solver information\n",
    "        \"\"\"\n",
    "        if warm_start_weights is not None:\n",
    "            weights = warm_start_weights.copy()\n",
    "        else:\n",
    "            if isinstance(init_weights,str):\n",
    "                weights = self.weights_setup(mode=init_weights)\n",
    "            else:\n",
    "                try:\n",
    "                    weights = init_weights(self)\n",
    "                    # TO DO full assertive statements \n",
    "                    assert sum([len(weight) for weight in weights])==self.objective.num_exprs,'user prescribed weights should match objective SumMinExpr.num_exprs'\n",
    "                    for elem in weights:\n",
    "                        if len(elem.shape)==1:\n",
    "                            assert np.min(elem)>=-1e-8 and abs(1-np.sum(elem))<=1e-8,'every weight vector should be POSITIVE and sum up to 1'\n",
    "                        elif len(elem.shape)==2:\n",
    "                            assert np.min(elem)>=-1e-8 and np.max(np.abs(1-np.sum(elem,1)))<=1e-8,'every weight vector should be POSITIVE and sum up to 1'\n",
    "                        else:\n",
    "                            raise ValueError('tensor like parameters not accepted yet...')\n",
    "                except:\n",
    "                    print('init_weights as a callable should take a smc.Problem argument | equiv. weights loaded...')\n",
    "                    weights = self.weights_setup()\n",
    "        \n",
    "        cvx_param_obj_prob = cp.Problem(cp.Minimize(self.param_obj_fun), self.constraints)\n",
    "        \n",
    "        out_val = []\n",
    "        \n",
    "        last_val = np.inf\n",
    "        decr = np.inf\n",
    "        \n",
    "        if C_sched is None:\n",
    "            C = lambda k: 2/(k**(1/2)+3) \n",
    "        else:\n",
    "            C = lambda k: C_sched(k)\n",
    "            \n",
    "        if param_sched is None:\n",
    "            kap = lambda k: kappa\n",
    "        else:\n",
    "            kap = lambda k: param_sched(k)\n",
    "            \n",
    "        for k in range(maxIters):\n",
    "            # x step, skipped if warm_start=True and k=0\n",
    "            if not warm_start or k > 0:\n",
    "                try:\n",
    "                    ## param affectation + solve\n",
    "                    if self.custom_set:\n",
    "                        params = self.w2p(weights)\n",
    "                        for param,param_pointer in zip(params,self.param_pointers_list):\n",
    "                            param_pointer.value = param\n",
    "                    else:\n",
    "                        for weight,param_pointer in zip(weights,self.param_pointers_list):\n",
    "                            param_pointer.value = np.maximum(0,weight)\n",
    "                except:\n",
    "                    print('numerical instability for weights/params setting...')\n",
    "                    return {'iters':k,'stopping_condition':decr,'objective_values':out_val}\n",
    "                try:\n",
    "                    cvx_param_obj_prob.solve(**kwargs)\n",
    "                except:\n",
    "                    print('solver issues... or manual STOP')\n",
    "                    print(cvx_param_obj_prob.value)\n",
    "                prob_value = cvx_param_obj_prob.value\n",
    "                decr = max(0.0,last_val-prob_value)\n",
    "                if cvx_param_obj_prob.status in ['unbounded','infeasible']:\n",
    "                    raise ValueError(\"weights-fixed problem is %s.\" % cvx_param_obj_prob.status)\n",
    "            else:\n",
    "                prob_value = self.objective.value\n",
    "                    \n",
    "            last_val = prob_value\n",
    "            true_val = self.objective.value\n",
    "            out_val.append(true_val)\n",
    "\n",
    "            if verb_:\n",
    "                print(\"iter. %04d | Fval. %4.4e | BICval. %4.4e\" % (k + 1, true_val,prob_value))\n",
    "                if extra_verb_ and (k>0 or not warm_start):\n",
    "                    print('used weights: '+str(weights))\n",
    "                    if self.custom_set:\n",
    "                        print('used params: '+str(params))\n",
    "                    print('variables: '+str([var.value for var in self.vars_]))\n",
    "                    \n",
    "            # weights step\n",
    "            if warm_start:\n",
    "                new_weights = self.weights_update(mode,weights,0,kap(k)) # forced full alternating-minimization \n",
    "            else:\n",
    "                new_weights = self.weights_update(mode,weights,C(k),kap(k))\n",
    "                \n",
    "            if decr < tol and true_val<out_val[0]-TOL_has_moved:\n",
    "                if verb_:\n",
    "                    print (\"-> terminated (stopping condition satisfied)\")\n",
    "                break\n",
    "            else:\n",
    "                TOL_has_moved = max(-1e-9,TOL_has_moved*2/3-1e-9)\n",
    "                weights = new_weights.copy()\n",
    "                \n",
    "        if verb_ and k == maxIters-1:\n",
    "            print (\"-> terminated (maximum number of iterations reached)\")\n",
    "            \n",
    "        return {'iters':k+1,'stopping_condition':decr,'objective_values':out_val}       \n",
    "    \n",
    "    @property \n",
    "    def value(self):\n",
    "        for cstr in self.constraints:\n",
    "            if cstr.value()==False:\n",
    "                return np.inf # infeasibility\n",
    "        return self.objective.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
